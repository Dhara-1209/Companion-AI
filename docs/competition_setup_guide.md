# CompanionAI - Competition Compliant Setup Guide

## üèÜ **Competition Requirements Met**

‚úÖ **Local Models Only**: Uses Ollama for local inference  
‚úÖ **Nvidia NIM Support**: Ready for edge deployment  
‚úÖ **No Cloud APIs**: Completely removed Groq and other cloud dependencies  
‚úÖ **8GB RAM Compatible**: Optimized for resource-constrained environments  
‚úÖ **RAG Pipeline**: Uses FAISS vector search with manual embeddings  
‚úÖ **Safety System**: Built-in emergency detection and response  

---

## üöÄ **Quick Start (Competition Mode)**

### **Option 1: Local Ollama (Recommended for 8GB)**

1. **Install Ollama**:
   ```bash
   # Download from https://ollama.ai/download
   # Or use package manager
   winget install Ollama.Ollama
   ```

2. **Pull optimal models**:
   ```bash
   ollama pull phi3:mini      # 3.8GB - Best balance
   ollama pull gemma2:2b      # 1.6GB - Fastest
   ollama pull llama3.2:3b    # 2GB - Good quality
   ```

3. **Start Ollama server**:
   ```bash
   ollama serve
   ```

### **Option 2: Nvidia NIM (Cloud/Edge)**

1. **Get Nvidia API Key**: Sign up at [build.nvidia.com](https://build.nvidia.com)

2. **Configure environment**:
   ```bash
   # Add to .env file
   NVIDIA_API_KEY=your_nvidia_api_key_here
   ```

### **Option 3: Template Mode (No AI Dependencies)**

The system works without any AI models using intelligent templates based on RAG context.

---

## üõ† **Installation & Running**

### **Backend (FastAPI)**:
```bash
# Install dependencies
pip install -r requirements.txt

# Start API server
uvicorn src.backend.main:app --host 127.0.0.1 --port 8000 --reload
```

### **Frontend (Streamlit)**:
```bash
# Start web interface
streamlit run src/frontend/app.py --server.port 8502
```

### **Access Application**:
- **Frontend**: http://localhost:8502
- **API Docs**: http://127.0.0.1:8000/docs

---

## ‚ö° **Performance Expectations**

### **Local Models (8GB RAM)**:
| Model | Size | Cold Start | Response | Quality |
|-------|------|------------|----------|---------|
| phi3:mini | 3.8GB | 15-20s | 8-12s | ‚≠ê‚≠ê‚≠ê‚≠ê |
| gemma2:2b | 1.6GB | 8-12s | 3-6s | ‚≠ê‚≠ê‚≠ê |
| llama3.2:3b | 2GB | 10-15s | 5-8s | ‚≠ê‚≠ê‚≠ê‚≠ê |

### **Nvidia NIM**:
- **Response Time**: 1-3 seconds
- **Quality**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **Resource Usage**: Minimal local RAM

### **Template Mode**:
- **Response Time**: Instant
- **Quality**: Good for basic troubleshooting
- **Resource Usage**: <100MB RAM

---

## üéØ **Competition Demo Script**

### **1. System Overview**
"CompanionAI is a RAG-powered appliance troubleshooting assistant that complies with competition requirements by using only local models or Nvidia NIM."

### **2. Show Local Model**
```bash
# Demonstrate Ollama status
curl http://localhost:11434/api/tags

# Show model selection
# System automatically chooses phi3:mini for 8GB RAM
```

### **3. Test Safety Detection**
**Input**: "I smell gas from my oven"  
**Output**: Immediate safety alert with evacuation instructions

### **4. Show RAG Retrieval**
**Input**: "How to clean LG microwave filter"  
**Output**: Contextual response using manual excerpts

### **5. Demonstrate Performance**
**Show response times**:
- Emergency queries: 3-8 seconds (local) or 1-2s (NIM)
- Regular queries: 5-12 seconds (local) or 1-3s (NIM)
- Template fallback: Instant

### **6. Competition Compliance**
- ‚úÖ No Groq or OpenAI cloud APIs
- ‚úÖ Works completely offline with Ollama
- ‚úÖ Nvidia NIM support for better performance
- ‚úÖ 8GB RAM compatibility proven
- ‚úÖ Safety-first approach with emergency detection

---

## üèó **Architecture Highlights**

### **Smart Model Router**:
```python
# Automatically selects best approach
router = CompetitionCompliantRouter()

# Emergency ‚Üí Fastest available
# Privacy-sensitive ‚Üí Local Ollama  
# Complex queries ‚Üí Nvidia NIM
# Simple queries ‚Üí Local sufficient
```

### **Enhanced Ollama Client**:
- Multi-model support with automatic selection
- Optimized parameters per model type
- Error handling and fallbacks
- Performance monitoring

### **Competition-Safe Fallbacks**:
1. **Primary**: Local Ollama models
2. **Secondary**: Nvidia NIM (if configured)
3. **Fallback**: Intelligent RAG-based templates
4. **Emergency**: Safety-focused responses

---

## üìä **System Status Check**

```python
# Get current system capabilities
from src.core.models.smart_router import CompetitionCompliantRouter

router = CompetitionCompliantRouter()
status = router.get_system_status()

print(status)
# {
#   "ollama_available": True,
#   "nvidia_nim_available": False, 
#   "competition_compliant": True,
#   "cloud_apis_disabled": True
# }
```

---

## üé™ **Demo Scenarios**

### **1. Emergency Safety**
**Query**: "Sparks coming from my washing machine"  
**Response**: Immediate electrical safety protocol

### **2. Local Privacy**
**Query**: "Check my appliance warranty status"  
**Response**: Uses local Ollama to keep data private

### **3. Technical Troubleshooting**
**Query**: "Step-by-step dishwasher pump replacement"  
**Response**: Detailed instructions using manual context

### **4. Simple Maintenance**
**Query**: "How often should I clean my dryer lint?"  
**Response**: Quick answer from local model or template

---

## üèÜ **Competition Advantages**

1. **100% Compliant**: No cloud API dependencies
2. **Resource Efficient**: Works on 8GB laptops  
3. **Always Available**: Offline functionality guaranteed
4. **Safety First**: Emergency detection built-in
5. **Flexible**: Supports both Ollama and Nvidia NIM
6. **User-Friendly**: Streamlit interface with clear feedback
7. **Production Ready**: FastAPI backend with proper error handling

---

## üîß **Troubleshooting**

### **Ollama Not Working**:
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama service
ollama serve

# Check available models
ollama list
```

### **High Memory Usage**:
```bash
# Use smaller model
ollama pull gemma2:2b

# Monitor usage
Task Manager ‚Üí Check RAM usage
```

### **Slow Responses**:
- **First query**: 15-20s (model loading)
- **Subsequent**: 5-10s (normal)
- **Solution**: Keep Ollama running, use SSD storage

---

This setup ensures **100% competition compliance** while delivering excellent appliance troubleshooting capabilities! üöÄ